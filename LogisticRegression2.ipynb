{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea98524b-98d2-4234-92c5-1e7e1a49c2e0",
   "metadata": {},
   "source": [
    "#### Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88358a70-8d87-4b57-8bc5-872876832831",
   "metadata": {},
   "source": [
    "Ans--> The purpose of grid search cross-validation (GridSearchCV) in machine learning is to systematically search for the optimal combination of hyperparameters for a given model. Hyperparameters are adjustable parameters that are not learned from the data but are set prior to training the model.\n",
    "\n",
    "GridSearchCV works by exhaustively searching a specified grid of hyperparameter values to determine the combination that yields the best model performance. It performs a cross-validation evaluation for each combination of hyperparameters and returns the combination that achieves the highest performance score.\n",
    "\n",
    "Here's how GridSearchCV works step by step:\n",
    "\n",
    "1. Define the model: Select the machine learning algorithm you want to use and define its structure and settings.\n",
    "\n",
    "2. Specify the hyperparameter grid: Determine the hyperparameters you want to tune and specify the range or set of values for each hyperparameter. GridSearchCV will create a grid of all possible combinations of these values.\n",
    "\n",
    "3. Cross-validation setup: Specify the cross-validation strategy to be used, such as k-fold cross-validation. This determines how the data will be split into training and validation subsets.\n",
    "\n",
    "4. Perform grid search: GridSearchCV iterates through each combination of hyperparameters and evaluates the model's performance using cross-validation. It trains and evaluates the model multiple times, each time using a different combination of hyperparameters.\n",
    "\n",
    "5. Evaluation: For each combination of hyperparameters, GridSearchCV calculates the performance metric (such as accuracy, F1-score, or AUC) using the cross-validation results. It keeps track of the best combination of hyperparameters based on the specified performance metric.\n",
    "\n",
    "6. Select the best model: Once GridSearchCV has evaluated all combinations, it selects the combination of hyperparameters that yielded the best performance score. This best-performing model can then be used for making predictions on new, unseen data.\n",
    "\n",
    "GridSearchCV provides an automated and efficient way to search for the optimal hyperparameters without manually testing each combination. By systematically exploring the hyperparameter space and using cross-validation, it helps in finding the hyperparameters that generalize well to unseen data and optimize the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265f404-8510-4309-aff5-d632ee586ac7",
   "metadata": {},
   "source": [
    "#### Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a20821-f57c-479c-a929-bd4a9e7581b2",
   "metadata": {},
   "source": [
    "Ans--> GridSearchCV and RandomizedSearchCV are both techniques used for hyperparameter optimization, but they differ in their search strategies and the way they explore the hyperparameter space.\n",
    "\n",
    "**GridSearchCV** performs an exhaustive search over all possible combinations of hyperparameter values specified in a grid. It evaluates and compares the model's performance for each combination using cross-validation. GridSearchCV is deterministic, meaning it tries every combination and does not involve randomness.\n",
    "\n",
    "**RandomizedSearchCV** randomly samples hyperparameter combinations from a specified distribution of possible values. It allows you to define a search space for each hyperparameter, and the search is guided by random sampling. RandomizedSearchCV evaluates a subset of the total possible combinations, based on the specified number of iterations, and performs cross-validation for each sampled combination.\n",
    "\n",
    "Here are some considerations for choosing between GridSearchCV and RandomizedSearchCV:\n",
    "\n",
    "1. **Search Space Size**: GridSearchCV exhaustively searches the entire grid of possible hyperparameter combinations. This approach can be computationally expensive when dealing with a large search space or a high number of hyperparameters. In such cases, RandomizedSearchCV is more efficient as it samples a subset of combinations.\n",
    "\n",
    "2. **Computational Resources**: GridSearchCV requires more computational resources since it explores the entire grid. If you have limited computational resources or time constraints, RandomizedSearchCV can be a suitable alternative.\n",
    "\n",
    "3. **Hyperparameter Importance**: If you have a priori knowledge or strong intuition about the importance of specific hyperparameters, GridSearchCV allows you to specifically target those hyperparameters by setting a narrower grid search range. RandomizedSearchCV, on the other hand, randomly samples from the defined distribution, which can explore a wider range of hyperparameters.\n",
    "\n",
    "4. **Exploration vs. Exploitation**: RandomizedSearchCV is more suited for exploration of the hyperparameter space. It can help discover potential hyperparameter configurations that may not have been considered initially. GridSearchCV, on the other hand, is more focused on exploitation, thoroughly searching the specified grid.\n",
    "\n",
    "In summary, GridSearchCV is suitable when the search space is relatively small and computational resources are sufficient. It is effective when you want to exhaustively search all combinations or when you have specific hyperparameters that you want to tune precisely. RandomizedSearchCV is useful when the search space is large, and you want to explore a wider range of hyperparameters efficiently, making it more suitable for an initial exploration of the hyperparameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b969e5-534e-439f-88a1-d7ec90a97c4f",
   "metadata": {},
   "source": [
    "#### Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6184b1-f0c5-4b3d-985d-85a6354b4f81",
   "metadata": {},
   "source": [
    "Ans--> Data leakage refers to the situation where information from outside the training data is unintentionally or inappropriately used to create or evaluate a machine learning model. It occurs when there is a leakage of information from the test set or future data into the training or validation process, leading to overly optimistic performance metrics or incorrect generalization.\n",
    "\n",
    "Data leakage is a problem in machine learning because it can result in models that perform well on the training or validation data but fail to generalize to new, unseen data. It undermines the reliability and effectiveness of the model, leading to incorrect decisions and potentially harmful consequences in real-world applications.\n",
    "\n",
    "Here's an example to illustrate data leakage:\n",
    "\n",
    "Suppose you are building a credit card fraud detection model. The dataset contains information about transactions, including the transaction amount, location, time, and a binary target variable indicating whether the transaction is fraudulent or not.\n",
    "\n",
    "Data leakage can occur in the following ways:\n",
    "\n",
    "1. **Using Future Information**: Let's say you have a feature called \"Is_Fraudulent\" that indicates whether a transaction is fraudulent or not. If you include this feature in your training data and use it to predict fraud, you would be using information that is not available at the time of the transaction. This is data leakage because in real-world scenarios, you wouldn't have access to the target variable at the time of making predictions.\n",
    "\n",
    "2. **Leaking Target Information**: If you mistakenly include features that are derived from or highly correlated with the target variable in the training data, it can lead to data leakage. For example, including the \"Fraud_Risk_Score\" calculated using future knowledge of fraud can introduce leakage, as the model could unintentionally learn the relationship between this feature and the target variable.\n",
    "\n",
    "3. **Time-based Leakage**: In time-series data, data leakage can occur when using information from the future to predict the past. For example, if you use transaction features from the future to predict whether a transaction was fraudulent in the past, it would result in data leakage and invalidate the model's real-world usefulness.\n",
    "\n",
    "The consequences of data leakage can be misleadingly high model accuracy, overly optimistic performance metrics, and models that fail to generalize to new data. To prevent data leakage, it is essential to carefully partition the data into training, validation, and test sets, ensure that features and information used in the model are realistic and available at the time of prediction, and rigorously evaluate the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b0b7d-41ec-40d0-b75c-547743f6ec5b",
   "metadata": {},
   "source": [
    "#### Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b51a3-c8bf-403c-ba06-78688e69bfeb",
   "metadata": {},
   "source": [
    "Ans--> To prevent data leakage when building a machine learning model, you can take several precautions:\n",
    "\n",
    "1. **Separate Training, Validation, and Test Sets**: Split your dataset into distinct subsets for training, validation, and testing. Ensure that no data from the validation or test sets is used in any way during the training phase to prevent any leakage of information.\n",
    "\n",
    "2. **Temporal Validation**: If working with time-series data, make sure to use a temporal validation strategy. Train the model on data from earlier time periods and validate it on more recent data. This ensures that the model does not have access to future information during training.\n",
    "\n",
    "3. **Feature Engineering**: Be cautious when creating features to avoid using information that would not be available in real-world scenarios. Do not use future knowledge or information from the target variable to create features.\n",
    "\n",
    "4. **Preprocessing**: Apply preprocessing techniques, such as scaling or normalization, separately to the training and validation/test sets. Do not calculate any statistics (e.g., mean, standard deviation) based on the entire dataset, as it may leak information from the validation or test sets into the training process.\n",
    "\n",
    "5. **Pipeline Construction**: Construct a pipeline that includes all preprocessing steps, feature selection, and model training. Ensure that any transformations or calculations are applied only to the training data and then transferred to the validation and test sets without re-estimating any parameters.\n",
    "\n",
    "6. **Nested Cross-Validation**: Use nested cross-validation when tuning hyperparameters. This technique involves performing an outer cross-validation loop for model evaluation and an inner cross-validation loop for hyperparameter tuning. This ensures that the hyperparameter selection process does not utilize information from the validation or test sets.\n",
    "\n",
    "7. **Careful Evaluation**: Evaluate the model's performance on the test set only after all model tuning and hyperparameter selection have been completed. Avoid making any adjustments to the model based on the test set performance.\n",
    "\n",
    "8. **Domain Knowledge**: Acquire a deep understanding of the data and the problem domain to identify potential sources of leakage. Consider consulting with domain experts to ensure that the model is built with accurate and realistic features.\n",
    "\n",
    "By following these steps, you can minimize the risk of data leakage and ensure that your machine learning model is trained and evaluated properly on independent and representative datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99860b3-4671-4290-856f-b691c81a7d5c",
   "metadata": {},
   "source": [
    "#### Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63365dc5-05ec-48a1-996f-48ec3846bf07",
   "metadata": {},
   "source": [
    "Ans--> A confusion matrix, also known as an error matrix, is a table that summarizes the performance of a classification model on a set of test data. It provides a detailed breakdown of the predicted and actual classes, allowing for the evaluation of the model's performance.\n",
    "\n",
    "A confusion matrix is typically organized as follows:\n",
    "\n",
    "```\n",
    "                Predicted Class\n",
    "               |    Positive    |   Negative   |\n",
    "-----------------------------------------------\n",
    "Actual Class   |                |              |\n",
    "Positive       | True Positive  | False Negative|\n",
    "(Negative)     |                |              |\n",
    "-----------------------------------------------\n",
    "Actual Class   |                |              |\n",
    "Negative       | False Positive | True Negative |\n",
    "(Negative)     |                |              |\n",
    "-----------------------------------------------\n",
    "```\n",
    "\n",
    "The confusion matrix consists of four important metrics:\n",
    "\n",
    "1. **True Positives (TP)**: The number of instances correctly predicted as positive (correctly classified as the positive class).\n",
    "\n",
    "2. **False Positives (FP)**: The number of instances incorrectly predicted as positive (incorrectly classified as the positive class).\n",
    "\n",
    "3. **True Negatives (TN)**: The number of instances correctly predicted as negative (correctly classified as the negative class).\n",
    "\n",
    "4. **False Negatives (FN)**: The number of instances incorrectly predicted as negative (incorrectly classified as the negative class).\n",
    "\n",
    "Based on these metrics, several evaluation measures can be calculated:\n",
    "\n",
    "- **Accuracy**: The overall accuracy of the model, calculated as (TP + TN) / (TP + TN + FP + FN). It represents the proportion of correctly classified instances out of the total number of instances.\n",
    "\n",
    "- **Precision**: Also known as positive predictive value, it is calculated as TP / (TP + FP). Precision measures the accuracy of the positive predictions, indicating the proportion of correctly predicted positive instances out of all predicted positive instances.\n",
    "\n",
    "- **Recall**: Also known as sensitivity or true positive rate, it is calculated as TP / (TP + FN). Recall measures the model's ability to correctly identify positive instances out of all actual positive instances.\n",
    "\n",
    "- **F1-Score**: The harmonic mean of precision and recall, calculated as 2 * (Precision * Recall) / (Precision + Recall). It provides a balanced measure of the model's performance, considering both precision and recall.\n",
    "\n",
    "The confusion matrix allows for a more detailed evaluation of a classification model's performance by providing insights into different types of errors made by the model. It helps identify whether the model is more prone to false positives (Type I errors) or false negatives (Type II errors) and provides valuable information for adjusting the model or setting the decision threshold based on the specific requirements and constraints of the problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c7558f-54b3-442d-b34e-3fa62d460901",
   "metadata": {},
   "source": [
    "#### Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeddc9b-9e72-4b80-80c4-7f577cb7971f",
   "metadata": {},
   "source": [
    "Ans--> In the context of a confusion matrix, precision and recall are two important metrics used to evaluate the performance of a classification model. They provide insights into different aspects of the model's predictive ability, specifically related to the positive class.\n",
    "\n",
    "- **Precision**: Precision is a measure of the accuracy of the positive predictions made by the model. It is calculated as the ratio of true positives (TP) to the sum of true positives and false positives (FP). Mathematically, precision is defined as TP / (TP + FP). \n",
    "\n",
    "Precision indicates the proportion of correctly predicted positive instances out of all instances predicted as positive. It focuses on the correctness of the positive predictions, regardless of the number of actual positive instances. A high precision indicates that when the model predicts an instance as positive, it is likely to be correct.\n",
    "\n",
    "- **Recall**: Recall, also known as sensitivity or true positive rate, measures the model's ability to correctly identify positive instances out of all actual positive instances. It is calculated as the ratio of true positives (TP) to the sum of true positives and false negatives (FN). Mathematically, recall is defined as TP / (TP + FN).\n",
    "\n",
    "Recall quantifies the model's effectiveness in capturing all positive instances. It provides an understanding of how well the model performs in identifying the actual positive instances, irrespective of any negative instances that may have been incorrectly predicted as positive. A high recall indicates that the model is successful in capturing a large proportion of positive instances.\n",
    "\n",
    "To summarize:\n",
    "- Precision focuses on the accuracy of positive predictions, considering the proportion of correctly predicted positive instances out of all instances predicted as positive.\n",
    "- Recall measures the ability of the model to correctly identify positive instances, considering the proportion of correctly predicted positive instances out of all actual positive instances.\n",
    "\n",
    "Both precision and recall are important metrics in different contexts. The choice between them depends on the specific requirements and constraints of the problem. In some cases, precision might be more critical (e.g., in fraud detection, where false positives are costly), while in other cases, recall might be more important (e.g., in medical diagnoses, where false negatives can be more detrimental)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c9129-7f8f-4623-a7e2-faf38f91f65a",
   "metadata": {},
   "source": [
    "#### Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f156e-1cbe-4492-b4a7-0d2b01c3e2ae",
   "metadata": {},
   "source": [
    "Ans--> Interpreting a confusion matrix can provide valuable insights into the types of errors your classification model is making. By examining the values within the matrix, you can determine the specific types of errors and their frequencies. Here's how you can interpret a confusion matrix:\n",
    "\n",
    "1. **True Positives (TP)**: This represents the number of instances correctly predicted as positive (correctly classified as the positive class). These are the cases where the model correctly identified the positive class.\n",
    "\n",
    "2. **False Positives (FP)**: This indicates the number of instances incorrectly predicted as positive (incorrectly classified as the positive class). These are the cases where the model predicted the positive class, but the actual class was negative. False positives are also known as Type I errors.\n",
    "\n",
    "3. **True Negatives (TN)**: This represents the number of instances correctly predicted as negative (correctly classified as the negative class). These are the cases where the model correctly identified the negative class.\n",
    "\n",
    "4. **False Negatives (FN)**: This indicates the number of instances incorrectly predicted as negative (incorrectly classified as the negative class). These are the cases where the model predicted the negative class, but the actual class was positive. False negatives are also known as Type II errors.\n",
    "\n",
    "To interpret the confusion matrix:\n",
    "\n",
    "- High TP: A high number of true positives indicates that the model is performing well in correctly predicting positive instances.\n",
    "\n",
    "- High TN: A high number of true negatives indicates that the model is performing well in correctly predicting negative instances.\n",
    "\n",
    "- High FP: A high number of false positives indicates that the model is making more Type I errors, falsely predicting positive instances when they are actually negative. This could suggest that the model has a higher tendency to generate false alarms or false positives.\n",
    "\n",
    "- High FN: A high number of false negatives indicates that the model is making more Type II errors, failing to predict positive instances correctly. This could imply that the model is missing positive cases or has a higher rate of false negatives.\n",
    "\n",
    "By analyzing the distribution of TP, FP, TN, and FN, you can gain insights into the specific types of errors your model is making. Understanding these errors can help you fine-tune the model, adjust the decision threshold, or consider different evaluation metrics based on the specific requirements and constraints of your problem domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6354098-62ff-4bb8-9d04-918c812fe6a1",
   "metadata": {},
   "source": [
    "#### Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa25294-f05d-4da6-af8d-c598a9370b4c",
   "metadata": {},
   "source": [
    "Ans--> Several common metrics can be derived from a confusion matrix to evaluate the performance of a classification model. These metrics provide insights into different aspects of the model's performance. Here are some commonly used metrics:\n",
    "\n",
    "1. **Accuracy**: Accuracy measures the overall correctness of the model's predictions. It is calculated as (TP + TN) / (TP + TN + FP + FN). Accuracy represents the proportion of correctly classified instances out of the total number of instances.\n",
    "\n",
    "2. **Precision**: Precision is the measure of the accuracy of positive predictions made by the model. It is calculated as TP / (TP + FP). Precision indicates the proportion of correctly predicted positive instances out of all instances predicted as positive.\n",
    "\n",
    "3. **Recall**: Recall, also known as sensitivity or true positive rate, measures the model's ability to correctly identify positive instances. It is calculated as TP / (TP + FN). Recall indicates the proportion of correctly predicted positive instances out of all actual positive instances.\n",
    "\n",
    "4. **Specificity**: Specificity, also known as true negative rate, measures the model's ability to correctly identify negative instances. It is calculated as TN / (TN + FP). Specificity indicates the proportion of correctly predicted negative instances out of all actual negative instances.\n",
    "\n",
    "5. **F1-Score**: The F1-score is the harmonic mean of precision and recall. It provides a balanced measure of the model's performance, considering both precision and recall. The F1-score is calculated as 2 * (Precision * Recall) / (Precision + Recall).\n",
    "\n",
    "6. **False Positive Rate (FPR)**: FPR measures the proportion of actual negative instances that are incorrectly classified as positive. It is calculated as FP / (FP + TN). FPR is the complement of specificity.\n",
    "\n",
    "7. **False Negative Rate (FNR)**: FNR measures the proportion of actual positive instances that are incorrectly classified as negative. It is calculated as FN / (FN + TP). FNR is the complement of recall.\n",
    "\n",
    "These metrics provide different perspectives on the model's performance and can be used depending on the specific requirements and constraints of the problem. It's important to choose the appropriate metrics based on the nature of the problem, the importance of different types of errors, and the desired trade-offs between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70b4f37-5e75-45eb-954d-3c8d6d83b5fe",
   "metadata": {},
   "source": [
    "#### Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ed77b-ae1b-429d-ad81-57132d34667f",
   "metadata": {},
   "source": [
    "Ans--> The accuracy of a model is closely related to the values in its confusion matrix. The confusion matrix provides a detailed breakdown of the model's predictions and actual classes, and accuracy is calculated based on these values.\n",
    "\n",
    "Accuracy is defined as the proportion of correctly classified instances out of the total number of instances and is calculated as (TP + TN) / (TP + TN + FP + FN), where TP is the number of true positives, TN is the number of true negatives, FP is the number of false positives, and FN is the number of false negatives.\n",
    "\n",
    "The relationship between accuracy and the values in the confusion matrix can be understood as follows:\n",
    "\n",
    "1. True Positives (TP) and True Negatives (TN): These values contribute positively to the accuracy score. They represent the correct predictions made by the model for positive and negative instances, respectively. When these values are high, the accuracy of the model tends to increase.\n",
    "\n",
    "2. False Positives (FP) and False Negatives (FN): These values contribute negatively to the accuracy score. They represent the incorrect predictions made by the model for positive and negative instances, respectively. When these values are high, the accuracy of the model tends to decrease.\n",
    "\n",
    "In essence, accuracy measures the overall correctness of the model's predictions, taking into account both true positive and true negative predictions, as well as false positive and false negative predictions. It provides a general overview of the model's performance, considering all classes.\n",
    "\n",
    "However, it is important to note that accuracy alone may not be sufficient to assess the performance of a model, especially in cases of imbalanced datasets or when different types of errors have varying consequences. In such scenarios, other metrics such as precision, recall, F1-score, or specificity might provide a more comprehensive evaluation of the model's performance by considering specific types of errors and their implications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0bf16b-0ea0-48e0-a081-96cc99799a04",
   "metadata": {},
   "source": [
    "#### Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12eb14f-8e7a-4d4b-abf6-de34f210a54a",
   "metadata": {},
   "source": [
    "Ans--> A confusion matrix can be used to identify potential biases or limitations in your machine learning model by examining the distribution of predicted and actual classes. Here's how you can use a confusion matrix to gain insights into potential biases:\n",
    "\n",
    "1. **Class Imbalance**: Check if the confusion matrix shows a significant difference in the number of instances between classes. A highly imbalanced dataset, where one class dominates the other, may lead to biased predictions. The model may have a tendency to predict the majority class more frequently, resulting in lower performance on the minority class.\n",
    "\n",
    "2. **Misclassification Patterns**: Analyze the distribution of false positives (FP) and false negatives (FN) in the confusion matrix. Look for instances where one class is consistently misclassified as another class. This can indicate potential biases or limitations in the model's ability to discriminate between certain classes.\n",
    "\n",
    "3. **Disproportionate Errors**: Assess if the model makes more errors in predicting one class over another. A higher number of false positives for a particular class suggests a higher false alarm rate, while a higher number of false negatives indicates a higher miss rate. These disproportions may reveal biases or limitations in the model's decision boundaries.\n",
    "\n",
    "4. **Specificity and Sensitivity**: Examine the specificity (true negative rate) and sensitivity (true positive rate) values in the confusion matrix. If there is a significant difference in these values between classes, it may indicate biases in the model's ability to correctly identify positive and negative instances for different classes.\n",
    "\n",
    "5. **Evaluation across Subgroups**: If your dataset contains subgroups defined by certain characteristics (e.g., gender, age, race), evaluate the model's performance on each subgroup individually. This can help identify biases or limitations in the model's predictions that may disproportionately affect certain groups.\n",
    "\n",
    "By examining the patterns and values within the confusion matrix, you can identify potential biases or limitations in your model's predictions. It is essential to interpret the results critically and consider the context of the problem domain, the dataset, and any external factors that may contribute to the observed biases. Addressing these biases may require data preprocessing techniques, model adjustments, or the collection of more representative and diverse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311caf3-9b2f-4425-91ef-81b5e2eff438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
